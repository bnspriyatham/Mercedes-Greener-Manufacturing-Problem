{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.choices(list(test.index),k =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_point = test.iloc[random_index].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Preprocessing data using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns which need to be removed.\n",
    "removable_columns = pickle.load(open('Removable_col','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interaction features which need to be added.\n",
    "interaction_columns = pickle.load(open('Corr-features','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names which need to be added to make interaction features\n",
    "corr_columns = []\n",
    "for i in interaction_columns:\n",
    "    col = re.sub('_add_',' ',i)\n",
    "    corr_columns.append(col.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyathambns\\Anaconda3\\envs\\sora-ai\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator PCA from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\Priyathambns\\Anaconda3\\envs\\sora-ai\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Loading pca\n",
    "pca = pickle.load(open('models/pca.sav','rb'))\n",
    "#Loading pca scaler\n",
    "pca_scaler = pickle.load(open('models/pca-scaler.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading classes mean used for mean encoding.\n",
    "default_mean = pickle.load(open('class-means/default_mean','rb'))\n",
    "train_X0 = pickle.load(open('class-means/mean_X0','rb'))\n",
    "train_X1 = pickle.load(open('class-means/mean_X1','rb'))\n",
    "train_X2 = pickle.load(open('class-means/mean_X2','rb'))\n",
    "train_X3 = pickle.load(open('class-means/mean_X3','rb'))\n",
    "train_X5 = pickle.load(open('class-means/mean_X5','rb'))\n",
    "train_X6 = pickle.load(open('class-means/mean_X6','rb'))\n",
    "train_X8 = pickle.load(open('class-means/mean_X8','rb'))\n",
    "#Mean scaler\n",
    "mean_scaler = pickle.load(open('models/mean-scaler.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the best model from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyathambns\\Anaconda3\\envs\\sora-ai\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator ExtraTreeRegressor from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\Priyathambns\\Anaconda3\\envs\\sora-ai\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator ExtraTreesRegressor from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "mean_correlated_extratrees = pickle.load(open('models/mean_corr_pca_et_final.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(query):\n",
    "    \n",
    "    query = pd.DataFrame(query,columns = test.columns)\n",
    "    \n",
    "    #Removing the columns\n",
    "    query.drop(removable_columns,axis = 1,inplace = True)\n",
    "    \n",
    "    #Getting Remaining Numerical columns for pca transformation\n",
    "    numeric = list(train.select_dtypes(include = 'int64').columns)\n",
    "    numeric_col = []\n",
    "    for i in numeric:\n",
    "        if i in query.columns:\n",
    "            numeric_col.append(i)\n",
    "            \n",
    "    query_numeric = query[numeric_col]\n",
    "    query_numeric_norm = pca_scaler.transform(query_numeric)\n",
    "    query_numeric_norm = pd.DataFrame(query_numeric_norm,columns = query_numeric.columns)\n",
    "    \n",
    "    #Mean Encoding of categorical variables\n",
    "    query['X0'] = [train_X0.loc[i] if i in train_X0.index else default_mean for i in query['X0'].values]\n",
    "    query['X1'] = [train_X1.loc[i] if i in train_X1.index else default_mean for i in query['X1'].values]\n",
    "    query['X2'] = [train_X2.loc[i] if i in train_X2.index else default_mean for i in query['X2'].values]\n",
    "    query['X3'] = [train_X3.loc[i] if i in train_X3.index else default_mean for i in query['X3'].values]\n",
    "    query['X5'] = [train_X5.loc[i] if i in train_X5.index else default_mean for i in query['X5'].values]\n",
    "    query['X6'] = [train_X6.loc[i] if i in train_X6.index else default_mean for i in query['X6'].values]\n",
    "    query['X8'] = [train_X8.loc[i] if i in train_X8.index else default_mean for i in query['X8'].values]\n",
    "    \n",
    "    #Adding the interaction features\n",
    "    for cols in corr_columns: \n",
    "        if len(cols) == 2:\n",
    "            query[str(cols[0])+'_add_'+str(cols[1])] = query[cols[0]]+query[cols[1]]\n",
    "        else:\n",
    "            query[str(cols[0])+'_add_'+str(cols[1])+'_add_'+str(cols[2])] = query[cols[0]]+query[cols[1]]+query[cols[2]]\n",
    "    \n",
    "    query_norm = mean_scaler.transform(query)\n",
    "    query_norm = pd.DataFrame(query_norm,columns = query.columns)\n",
    "    \n",
    "    #Adding PCA features\n",
    "    query_pca = pca.transform(query_numeric_norm)\n",
    "    for i in range(1,7):\n",
    "        query_norm['pca'+str(i)] = query_pca[:,i-1]\n",
    "        query_norm['pca'+str(i)] = query_pca[:,i-1]\n",
    "    \n",
    "    #Predicting the target value\n",
    "    predicted = mean_correlated_extratrees.predict(query_norm)\n",
    "    return predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = prediction(query_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted target value for given query point is  75.86102607550015  seconds\n"
     ]
    }
   ],
   "source": [
    "print('The predicted target value for given query point is ',target_predicted,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking 10 sample points from the train dataset to calculate R2 score.\n",
    "random_index = random.choices(list(train.index),k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting random points from dataset which will be a numpy array\n",
    "random_points = train.iloc[random_index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(input):\n",
    "    \n",
    "    df = pd.DataFrame(input,columns = train.columns)\n",
    "    y_true = df['y'].values\n",
    "    df.drop(['y'],axis = 1,inplace = True)\n",
    "    \n",
    "    #Removing the columns\n",
    "    df.drop(removable_columns,axis = 1,inplace = True)\n",
    "    \n",
    "    #Getting Remaining Numerical columns for pca transformation\n",
    "    numeric = list(train.select_dtypes(include = 'int64').columns)\n",
    "    numeric_col = []\n",
    "    for i in numeric:\n",
    "        if i in df.columns:\n",
    "            numeric_col.append(i)\n",
    "\n",
    "    df_numeric = df[numeric_col]\n",
    "    df_numeric_norm = pca_scaler.transform(df_numeric)\n",
    "    df_numeric_norm = pd.DataFrame(df_numeric_norm,columns = df_numeric.columns)\n",
    "    \n",
    "    #Mean Encoding of categorical variables\n",
    "    df['X0'] = [train_X0.loc[i] for i in df['X0'].values]\n",
    "    df['X1'] = [train_X1.loc[i] for i in df['X1'].values]\n",
    "    df['X2'] = [train_X2.loc[i] for i in df['X2'].values]\n",
    "    df['X3'] = [train_X3.loc[i] for i in df['X3'].values]\n",
    "    df['X5'] = [train_X5.loc[i] for i in df['X5'].values]\n",
    "    df['X6'] = [train_X6.loc[i] for i in df['X6'].values]\n",
    "    df['X8'] = [train_X8.loc[i] for i in df['X8'].values]\n",
    "    \n",
    "    #Adding the interaction features\n",
    "    for cols in corr_columns: \n",
    "        if len(cols) == 2:\n",
    "            df[str(cols[0])+'_add_'+str(cols[1])] = df[cols[0]]+df[cols[1]]\n",
    "        else:\n",
    "            df[str(cols[0])+'_add_'+str(cols[1])+'_add_'+str(cols[2])] = df[cols[0]]+df[cols[1]]+df[cols[2]]\n",
    "    \n",
    "    df_norm = mean_scaler.transform(df)\n",
    "    df_norm = pd.DataFrame(df_norm,columns = df.columns)\n",
    "    \n",
    "    \n",
    "    #Adding PCA features\n",
    "    df_pca = pca.transform(df_numeric_norm)\n",
    "    for i in range(1,7):\n",
    "        df_norm['pca'+str(i)] = df_pca[:,i-1]\n",
    "        df_norm['pca'+str(i)] = df_pca[:,i-1]\n",
    "\n",
    "        \n",
    "    #Predicting the target value\n",
    "    y_pred = mean_correlated_extratrees.predict(df_norm)\n",
    "    \n",
    "    #Calculating r2 score:\n",
    "    R2_score = r2_score(y_true,y_pred)\n",
    "    \n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_score = metric(random_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for given points is  0.6830123625773366\n"
     ]
    }
   ],
   "source": [
    "print('The R2 score for given points is ',R2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
